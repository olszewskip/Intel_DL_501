{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Using LSTMs to Classify the 20 Newsgroups Data Set\n",
    "The 20 Newsgroups data set is a well known classification problem. The goal is to classify which newsgroup a particular post came from.  The 20 possible groups are:\n",
    "\n",
    "`comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\trec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\t\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.med\n",
    "sci.space\n",
    "misc.forsale\t\n",
    "talk.politics.misc\n",
    "talk.politics.guns\n",
    "talk.politics.mideast\t\n",
    "talk.religion.misc\n",
    "alt.atheism\n",
    "soc.religion.christian`\n",
    "\n",
    "As you can see, some pairs of groups may be quite similar while others are very different.\n",
    "\n",
    "The data is given as a designated training set of size 11314 and test set of size 7532.  The 20 categories are represented in roughly equal proportions, so the baseline accuracy is around 5%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, review the code below.  This will walk you through the basics of loading in the 20 newsgroups data, loading in the GloVe data, building the word embedding matrix, and building the LSTM model.\n",
    "\n",
    "After we build the first LSTM model, it will be your turn to build one and play with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "\n",
    "import keras\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "seq_length = 30  # How long to make our word sequences\n",
    "batch_size = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "# Download the 20 newsgroups data - there is already a designated \"train\" and \"test\" set\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 7532)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.data), len(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: lerxst@wam.umd'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(newsgroups_train.data)\n",
    "sequences_test = tokenizer.texts_to_sequences(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134142 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(sequences_train, maxlen=seq_length)\n",
    "x_test = pad_sequences(sequences_test, maxlen=seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(np.asarray(newsgroups_train.target))\n",
    "y_test = keras.utils.to_categorical(np.asarray(newsgroups_test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Glove pre-trained word vectors.  If you haven't already, please download them using this link:\n",
    "(NOTE: this will start downloading an 822MB file)\n",
    "\n",
    "http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "Then unzip the file and fill your local path to the file in the code cell below.\n",
    "\n",
    "We will use the file `glove.6B.100d.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just look at a word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30817  ,  0.30938  ,  0.52803  , -0.92543  , -0.73671  ,\n",
       "        0.63475  ,  0.44197  ,  0.10262  , -0.09142  , -0.56607  ,\n",
       "       -0.5327   ,  0.2013   ,  0.7704   , -0.13983  ,  0.13727  ,\n",
       "        1.1128   ,  0.89301  , -0.17869  , -0.0019722,  0.57289  ,\n",
       "        0.59479  ,  0.50428  , -0.28991  , -1.3491   ,  0.42756  ,\n",
       "        1.2748   , -1.1613   , -0.41084  ,  0.042804 ,  0.54866  ,\n",
       "        0.18897  ,  0.3759   ,  0.58035  ,  0.66975  ,  0.81156  ,\n",
       "        0.93864  , -0.51005  , -0.070079 ,  0.82819  , -0.35346  ,\n",
       "        0.21086  , -0.24412  , -0.16554  , -0.78358  , -0.48482  ,\n",
       "        0.38968  , -0.86356  , -0.016391 ,  0.31984  , -0.49246  ,\n",
       "       -0.069363 ,  0.018869 , -0.098286 ,  1.3126   , -0.12116  ,\n",
       "       -1.2399   , -0.091429 ,  0.35294  ,  0.64645  ,  0.089642 ,\n",
       "        0.70294  ,  1.1244   ,  0.38639  ,  0.52084  ,  0.98787  ,\n",
       "        0.79952  , -0.34625  ,  0.14095  ,  0.80167  ,  0.20987  ,\n",
       "       -0.86007  , -0.15308  ,  0.074523 ,  0.40816  ,  0.019208 ,\n",
       "        0.51587  , -0.34428  , -0.24525  , -0.77984  ,  0.27425  ,\n",
       "        0.22418  ,  0.20164  ,  0.017431 , -0.014697 , -1.0235   ,\n",
       "       -0.39695  , -0.0056188,  0.30569  ,  0.31748  ,  0.021404 ,\n",
       "        0.11837  , -0.11319  ,  0.42456  ,  0.53405  , -0.16717  ,\n",
       "       -0.27185  , -0.6255   ,  0.12883  ,  0.62529  , -0.52086  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_vec = embeddings_index['dog']\n",
    "dog_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in word_index.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This creates a matrix where the $i$th row gives the word embedding for the word represented by integer $i$.\n",
    "## Essentially, these will be the \"weights\" for the Embedding Layer\n",
    "## Rather than learning the weights, we will use these ones and \"freeze\" the layer\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134143, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Layer\n",
    "`keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
    "\n",
    "- Similar in structure to the `SimpleRNN` layer\n",
    "- `units` defines the dimension of the recurrent state\n",
    "- `recurrent_...` refers the recurrent state aspects of the LSTM\n",
    "- `kernel_...` refers to the transformations done on the input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 100)           13414300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 30)                15720     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                620       \n",
      "=================================================================\n",
      "Total params: 13,430,640\n",
      "Trainable params: 16,340\n",
      "Non-trainable params: 13,414,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_dimension = 100  # This is the dimension of the words we are using from GloVe\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            word_dimension,  \n",
    "                            weights=[embedding_matrix],  # We set the weights to be the word vectors from GloVe\n",
    "                            input_length=seq_length,\n",
    "                            trainable=False))  # By setting trainable to False, we \"freeze\" the word embeddings.\n",
    "model.add(LSTM(30, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(lr = .002)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 25s 2ms/step - loss: 2.7770 - acc: 0.1410 - val_loss: 2.4773 - val_acc: 0.2146\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 24s 2ms/step - loss: 2.3840 - acc: 0.2623 - val_loss: 2.2117 - val_acc: 0.3169\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 34s 3ms/step - loss: 2.1721 - acc: 0.3310 - val_loss: 2.0640 - val_acc: 0.3703\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 30s 3ms/step - loss: 2.0312 - acc: 0.3764 - val_loss: 1.9651 - val_acc: 0.4025\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 28s 2ms/step - loss: 1.9203 - acc: 0.4103 - val_loss: 1.9093 - val_acc: 0.4118\n",
      "Epoch 6/20\n",
      "11314/11314 [==============================] - 23s 2ms/step - loss: 1.8476 - acc: 0.4342 - val_loss: 1.8632 - val_acc: 0.4311\n",
      "Epoch 7/20\n",
      "11314/11314 [==============================] - 34s 3ms/step - loss: 1.7935 - acc: 0.4441 - val_loss: 1.8404 - val_acc: 0.4364\n",
      "Epoch 8/20\n",
      "11314/11314 [==============================] - 35s 3ms/step - loss: 1.7368 - acc: 0.4672 - val_loss: 1.8352 - val_acc: 0.4465\n",
      "Epoch 9/20\n",
      "11314/11314 [==============================] - 34s 3ms/step - loss: 1.6928 - acc: 0.4756 - val_loss: 1.8053 - val_acc: 0.4509\n",
      "Epoch 10/20\n",
      "11314/11314 [==============================] - 35s 3ms/step - loss: 1.6574 - acc: 0.4951 - val_loss: 1.8131 - val_acc: 0.4533\n",
      "Epoch 11/20\n",
      "11314/11314 [==============================] - 36s 3ms/step - loss: 1.6388 - acc: 0.4972 - val_loss: 1.7968 - val_acc: 0.4566\n",
      "Epoch 12/20\n",
      "11314/11314 [==============================] - 37s 3ms/step - loss: 1.6159 - acc: 0.5066 - val_loss: 1.7823 - val_acc: 0.4630\n",
      "Epoch 13/20\n",
      "11314/11314 [==============================] - 24s 2ms/step - loss: 1.5838 - acc: 0.5148 - val_loss: 1.7807 - val_acc: 0.4673\n",
      "Epoch 14/20\n",
      "11314/11314 [==============================] - 24s 2ms/step - loss: 1.5643 - acc: 0.5186 - val_loss: 1.7809 - val_acc: 0.4624\n",
      "Epoch 15/20\n",
      "11314/11314 [==============================] - 24s 2ms/step - loss: 1.5463 - acc: 0.5230 - val_loss: 1.7675 - val_acc: 0.4708\n",
      "Epoch 16/20\n",
      "11314/11314 [==============================] - 23s 2ms/step - loss: 1.5299 - acc: 0.5310 - val_loss: 1.7649 - val_acc: 0.4705\n",
      "Epoch 17/20\n",
      "11314/11314 [==============================] - 25s 2ms/step - loss: 1.5108 - acc: 0.5354 - val_loss: 1.7672 - val_acc: 0.4761\n",
      "Epoch 18/20\n",
      "11314/11314 [==============================] - 29s 3ms/step - loss: 1.5082 - acc: 0.5381 - val_loss: 1.7712 - val_acc: 0.4752\n",
      "Epoch 19/20\n",
      "11314/11314 [==============================] - 23s 2ms/step - loss: 1.4936 - acc: 0.5414 - val_loss: 1.7832 - val_acc: 0.4748\n",
      "Epoch 20/20\n",
      "11314/11314 [==============================] - 25s 2ms/step - loss: 1.4850 - acc: 0.5460 - val_loss: 1.7754 - val_acc: 0.4764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2bba0ffd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "### Your Turn\n",
    "- Build a neural network with a SimpleRNN instead of an LSTM (with other dimensions and parameters the same). How does the performance compare?\n",
    "- Use the LSTM above without the pretrained word vectors (randomly initialize the weights and have them be learned during the training process).  How does the performance compare?\n",
    "- Try different sequence lengths, and dimensions for the hidden state of the LSTM.  Can you improve the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 30, 100)           13414300  \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 30)                3930      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                620       \n",
      "=================================================================\n",
      "Total params: 13,418,850\n",
      "Trainable params: 4,550\n",
      "Non-trainable params: 13,414,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_dimension = 100  # This is the dimension of the words we are using from GloVe\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            word_dimension,  \n",
    "                            weights=[embedding_matrix],  # We set the weights to be the word vectors from GloVe\n",
    "                            input_length=seq_length,\n",
    "                            trainable=False))  # By setting trainable to False, we \"freeze\" the word embeddings.\n",
    "model.add(SimpleRNN(30, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(lr = .002)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 8s 692us/step - loss: 2.9903 - acc: 0.0955 - val_loss: 2.8857 - val_acc: 0.1338\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 7s 607us/step - loss: 2.8103 - acc: 0.1535 - val_loss: 2.8247 - val_acc: 0.1490\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 7s 633us/step - loss: 2.7267 - acc: 0.1847 - val_loss: 2.7286 - val_acc: 0.1751\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 13s 1ms/step - loss: 2.6787 - acc: 0.1995 - val_loss: 2.7131 - val_acc: 0.1860\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 28s 3ms/step - loss: 2.6458 - acc: 0.2125 - val_loss: 2.6499 - val_acc: 0.1930\n",
      "Epoch 6/20\n",
      "11314/11314 [==============================] - 16s 1ms/step - loss: 2.6361 - acc: 0.2177 - val_loss: 2.6820 - val_acc: 0.2050\n",
      "Epoch 7/20\n",
      "11314/11314 [==============================] - 12s 1ms/step - loss: 2.5972 - acc: 0.2267 - val_loss: 2.6206 - val_acc: 0.2140\n",
      "Epoch 8/20\n",
      "11314/11314 [==============================] - 12s 1ms/step - loss: 2.5852 - acc: 0.2294 - val_loss: 2.7574 - val_acc: 0.1897\n",
      "Epoch 9/20\n",
      "11314/11314 [==============================] - 13s 1ms/step - loss: 2.5807 - acc: 0.2300 - val_loss: 2.6343 - val_acc: 0.2118\n",
      "Epoch 10/20\n",
      "11314/11314 [==============================] - 12s 1ms/step - loss: 2.5504 - acc: 0.2442 - val_loss: 2.6315 - val_acc: 0.2135\n",
      "Epoch 11/20\n",
      "11314/11314 [==============================] - 10s 920us/step - loss: 2.5478 - acc: 0.2438 - val_loss: 2.6197 - val_acc: 0.2164\n",
      "Epoch 12/20\n",
      "11314/11314 [==============================] - 14s 1ms/step - loss: 2.5538 - acc: 0.2355 - val_loss: 2.6508 - val_acc: 0.2169\n",
      "Epoch 13/20\n",
      "11314/11314 [==============================] - 16s 1ms/step - loss: 2.5440 - acc: 0.2440 - val_loss: 2.5916 - val_acc: 0.2256\n",
      "Epoch 14/20\n",
      "11314/11314 [==============================] - 15s 1ms/step - loss: 2.5328 - acc: 0.2427 - val_loss: 2.6160 - val_acc: 0.2207\n",
      "Epoch 15/20\n",
      "11314/11314 [==============================] - 12s 1ms/step - loss: 2.5321 - acc: 0.2488 - val_loss: 2.6184 - val_acc: 0.2232\n",
      "Epoch 16/20\n",
      "11314/11314 [==============================] - 13s 1ms/step - loss: 2.5167 - acc: 0.2538 - val_loss: 2.6155 - val_acc: 0.2276\n",
      "Epoch 17/20\n",
      "11314/11314 [==============================] - 10s 888us/step - loss: 2.5172 - acc: 0.2499 - val_loss: 2.5946 - val_acc: 0.2314\n",
      "Epoch 18/20\n",
      "11314/11314 [==============================] - 12s 1ms/step - loss: 2.5051 - acc: 0.2598 - val_loss: 2.5877 - val_acc: 0.2387\n",
      "Epoch 19/20\n",
      "11314/11314 [==============================] - 14s 1ms/step - loss: 2.4871 - acc: 0.2575 - val_loss: 2.5685 - val_acc: 0.2330\n",
      "Epoch 20/20\n",
      "11314/11314 [==============================] - 13s 1ms/step - loss: 2.5110 - acc: 0.2588 - val_loss: 2.5816 - val_acc: 0.2400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2bba134a8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 30, 100)           13414300  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30)                15720     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                620       \n",
      "=================================================================\n",
      "Total params: 13,430,640\n",
      "Trainable params: 13,430,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_dimension = 100  # This is the dimension of the words we are using from GloVe\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                            word_dimension,  \n",
    "                           # weights=[embedding_matrix],  # We set the weights to be the word vectors from GloVe\n",
    "                            input_length=seq_length,\n",
    "                            trainable=True))  # By setting trainable to False, we \"freeze\" the word embeddings.\n",
    "model.add(LSTM(30, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = keras.optimizers.RMSprop(lr = .002)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 107s 9ms/step - loss: 2.6904 - acc: 0.1679 - val_loss: 2.4243 - val_acc: 0.2464\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 103s 9ms/step - loss: 1.9953 - acc: 0.3903 - val_loss: 2.0836 - val_acc: 0.3698\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 97s 9ms/step - loss: 1.4943 - acc: 0.5468 - val_loss: 1.9977 - val_acc: 0.4106\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 98s 9ms/step - loss: 1.1430 - acc: 0.6602 - val_loss: 1.9467 - val_acc: 0.4599\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 98s 9ms/step - loss: 0.8862 - acc: 0.7423 - val_loss: 1.9588 - val_acc: 0.4730\n",
      "Epoch 6/20\n",
      "11314/11314 [==============================] - 98s 9ms/step - loss: 0.6916 - acc: 0.8005 - val_loss: 2.0748 - val_acc: 0.4896\n",
      "Epoch 7/20\n",
      "11314/11314 [==============================] - 98s 9ms/step - loss: 0.5475 - acc: 0.8423 - val_loss: 2.1071 - val_acc: 0.4964\n",
      "Epoch 8/20\n",
      "11314/11314 [==============================] - 102s 9ms/step - loss: 0.4442 - acc: 0.8771 - val_loss: 2.1514 - val_acc: 0.5074\n",
      "Epoch 9/20\n",
      "11314/11314 [==============================] - 90s 8ms/step - loss: 0.3576 - acc: 0.9009 - val_loss: 2.1968 - val_acc: 0.5154\n",
      "Epoch 10/20\n",
      "11314/11314 [==============================] - 106s 9ms/step - loss: 0.2925 - acc: 0.9211 - val_loss: 2.2477 - val_acc: 0.5134\n",
      "Epoch 11/20\n",
      "11314/11314 [==============================] - 99s 9ms/step - loss: 0.2405 - acc: 0.9341 - val_loss: 2.3677 - val_acc: 0.5207\n",
      "Epoch 12/20\n",
      "11314/11314 [==============================] - 100s 9ms/step - loss: 0.2006 - acc: 0.9478 - val_loss: 2.3973 - val_acc: 0.5251\n",
      "Epoch 13/20\n",
      "11314/11314 [==============================] - 105s 9ms/step - loss: 0.1725 - acc: 0.9536 - val_loss: 2.4719 - val_acc: 0.5256\n",
      "Epoch 14/20\n",
      "11314/11314 [==============================] - 108s 10ms/step - loss: 0.1503 - acc: 0.9626 - val_loss: 2.4771 - val_acc: 0.5254\n",
      "Epoch 15/20\n",
      "11314/11314 [==============================] - 108s 10ms/step - loss: 0.1367 - acc: 0.9657 - val_loss: 2.5339 - val_acc: 0.5300\n",
      "Epoch 16/20\n",
      "11314/11314 [==============================] - 104s 9ms/step - loss: 0.1241 - acc: 0.9700 - val_loss: 2.5418 - val_acc: 0.5287\n",
      "Epoch 17/20\n",
      "11314/11314 [==============================] - 103s 9ms/step - loss: 0.1092 - acc: 0.9735 - val_loss: 2.5904 - val_acc: 0.5308\n",
      "Epoch 18/20\n",
      "11314/11314 [==============================] - 105s 9ms/step - loss: 0.0993 - acc: 0.9760 - val_loss: 2.6407 - val_acc: 0.5285\n",
      "Epoch 19/20\n",
      "11314/11314 [==============================] - 100s 9ms/step - loss: 0.1007 - acc: 0.9754 - val_loss: 2.6548 - val_acc: 0.5273\n",
      "Epoch 20/20\n",
      "11314/11314 [==============================] - 102s 9ms/step - loss: 0.0905 - acc: 0.9780 - val_loss: 2.6983 - val_acc: 0.5232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2902a9898>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
